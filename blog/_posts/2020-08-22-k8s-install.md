---
title: Kubernetes 环境安装
date: 2020-08-22
tags: 
  - k8s
author: Menfre
location: Shenzhen
---

使用 docker 和 docker-compose 也有一段时间了，终于挤出时间把 k8s 安排上了。:dog:

这里我在物理机上开了两台 ubuntu18 虚拟机用于搭建 k8s 环境。一台作为控制平面，一台作为工作机。

首先我们需要确保机器上安装有 docker，关于 docker 如何安装就不再赘述；直奔主题。

## 安装 k8s 组件

国内安装 k8s 还是十分不方便的，这里我们使用 aliyun apt source 来加速安装过程。


```shell
# 先切换为 root，接下来大部分流程都需要在 root 下面完成
sudo su
```

```shell
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF  
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

> 因为咱们是萌新，所以还是使用 kubeadm 这种傻瓜式工具来安装。

## kubeadm init

初始化 k8s 环境前我们需要先准备下。

```shell
# 关闭 swap
swapoff -a
```

设置 cgroup driver 为 systemd，默认 docker 使用的是 cgroupfs。

```shell
vim /etc/docker/daemon.json
```

插入如下 json:

```json
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
```

```shell
systemctl restart docker
```

默认 kubeadm 会到 google 仓库去拉取静态 pod 使用到的镜像，因为墙的关系，我们需要提前从别处拉取镜像。

这里我只找到一个合适的仓库：

```http
registry.aliyuncs.com/google_containers
```

接下我们我们先观察下 kubeadm 需要使用到的镜像列表:

```shell
menfre@master:~$ kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.20.2
k8s.gcr.io/kube-controller-manager:v1.20.2
k8s.gcr.io/kube-scheduler:v1.20.2
k8s.gcr.io/kube-proxy:v1.20.2
k8s.gcr.io/pause:3.2
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns:1.7.0
```

Ok, 我们知道 kubeadm 会用到哪些镜像后就可以提前拉取了，这里我们通过如下脚本来完成这个动作：

```shell
vim ~/pull.sh
```

```shell
#!/bin/bash
images=(
    kube-apiserver:v1.18.6
    kube-controller-manager:v1.18.6
    kube-scheduler:v1.18.6
    kube-proxy:v1.18.6
    pause:3.2
    etcd:3.4.3-0
    coredns:1.6.7
)

for imageName in ${images[@]} ; do
    docker pull googlecontainersmirror/$imageName
    docker tag googlecontainersmirror/$imageName k8s.gcr.io/$imageName
done
```

```shell
./pull.sh
```

这个脚本帮我们做了两件事，它帮我们拉取了目标镜像，然后通过 docker tag 将镜像的名字还原成 k8s.gcr.io 前缀。

开始 init：

```shell
kubeadm init --pod-network-cidr=10.244.0.0/16
```

> 切记这里一定要分配 pod 网段，不然接下来安装 flannel 网络插件将无法成功。

顺利的话你会见到如下信息，记得备份起来。

```sh
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.3.46:6443 --token huosj5.krvkm8dr9olvyutw \
    --discovery-token-ca-cert-hash sha256:609c8eb6dd96c62bc6f27e52fc9be91bfe08f80be99e7a5bae86ee65c4b3d675
```

> kubeadm init 大概帮我们完成如下事情。
>
> * 预检查，以确保当前机器环境满足安装条件
> * 设置 kubelet 并重启
> * 生成 ca 自签证书和服务器客户端证书用于 k8s 组件之间以及各节点之间身份识别
> * 配置 k8s 各组件以及管理员信息
> * 启动控制平面用到的三个静态 pod，api-server/controller-manager/scheduler
> * 启动本地 etcd 静态 pod
> * 上传 kubeadm/kubelet 配置为 ConfigMap
> * 上传证书到 kubeadm-certs
> * 标记当前节点为控制平面
> * 生成临时 token，便于其他节点通过 token 加入集群
> * 设置 kubelet 客户端证书为可用状态
> * 安装必要的插件，coredns/kube-proxy

## 安装 Pod 网络插件

安装插件前先配置下本地 hosts，确保 raw.githubusercontent.com 域名可用。

```txt
199.232.68.133 raw.githubusercontent.com
```

```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

Flannel pod 使用到的镜像会从仓库 quay.io 拉取，可能会有点慢，可以临时备个梯子加速下。

## kubeadm join

同样的，我们其他机器需要先安装好 docker，以及 k8s 各个组件。可以把 【安装 k8s 组件】章节的命令再运行一遍。

同样的 worker 节点也需要安装有 kube-proxy/pause 等网络 pod。

通过如下脚本安装：

```shell
vim ~/pull.sh
```

```shell
#!/bin/bash
images=(
    kube-proxy:v1.18.6
    pause:3.2
)

for imageName in ${images[@]} ; do
    docker pull googlecontainersmirror/$imageName
    docker tag googlecontainersmirror/$imageName k8s.gcr.io/$imageName
done
```

> worker 节点安装的 k8s 版本最好与 master 节点的保持一致。

将前面备份的关键信息黏贴到 worker 并执行 join 操作。

```shell
kubeadm join 192.168.3.46:6443 --token huosj5.krvkm8dr9olvyutw \
    --discovery-token-ca-cert-hash sha256:609c8eb6dd96c62bc6f27e52fc9be91bfe08f80be99e7a5bae86ee65c4b3d675
```

最后我们通过 kubectl get nodes 和 kubectl get --namespace kube-system pods 看下部署结果吧。

```shell
NAME     STATUS   ROLES    AGE     VERSION
master   Ready    master   3h59m   v1.18.6
worker   Ready    <none>   3h25m   v1.18.6
```

```shell
NAME                             READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-4bs7n         1/1     Running   0          3h59m
coredns-66bff467f8-dtfl9         1/1     Running   0          3h59m
etcd-master                      1/1     Running   0          3h59m
kube-apiserver-master            1/1     Running   0          3h59m
kube-controller-manager-master   1/1     Running   0          3h59m
kube-flannel-ds-amd64-5tzmx      1/1     Running   0          3h53m
kube-flannel-ds-amd64-ndpzg      1/1     Running   0          3h26m
kube-proxy-4kckl                 1/1     Running   0          3h59m
kube-proxy-bkjwt                 1/1     Running   0          3h26m
kube-scheduler-master            1/1     Running   0          3h59m
```





 
 <comment/> 
